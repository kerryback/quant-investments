{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kerryback/mgmt638/blob/main/notebooks/06c-random_forest_apply.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applying a Random Forest\n",
    "\n",
    "### MGMT 638: Data-Driven Investments: Equity\n",
    "### Kerry Back, Rice University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "- Create current features:\n",
    "  - Get data from SQL library.\n",
    "  - We only want most recent data, but go back a couple of years to compute momentum, growth rates, etc.\n",
    "  - Follow same procedure as in 5a-fundamentals.ipynb, but do not shift momentum, volatility, etc. forward.\n",
    "  - And do not keep return (return for prior week is not useful)\n",
    "- Apply saved random forest model to current data to form future predictions.\n",
    "- Use predictions to identify best and worst stocks today (maybe sector neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 753, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"c:\\Users\\kerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\pool\\base.py\", line 1004, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"c:\\Users\\kerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\dialects\\mssql\\base.py\", line 2792, in do_rollback\n",
      "    super(MSDialect, self).do_rollback(dbapi_connection)\n",
      "  File \"c:\\Users\\kerry\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sqlalchemy\\engine\\default.py\", line 683, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "  File \"src\\pymssql\\_pymssql.pyx\", line 316, in pymssql._pymssql.Connection.rollback\n",
      "  File \"src\\pymssql\\_pymssql.pyx\", line 300, in pymssql._pymssql.Connection.rollback\n",
      "  File \"src\\pymssql\\_mssql.pyx\", line 1055, in pymssql._mssql.MSSQLConnection.execute_non_query\n",
      "  File \"src\\pymssql\\_mssql.pyx\", line 1076, in pymssql._mssql.MSSQLConnection.execute_non_query\n",
      "  File \"src\\pymssql\\_mssql.pyx\", line 1250, in pymssql._mssql.MSSQLConnection.format_and_run_query\n",
      "  File \"src\\pymssql\\_mssql.pyx\", line 1788, in pymssql._mssql.check_cancel_and_raise\n",
      "  File \"src\\pymssql\\_mssql.pyx\", line 1834, in pymssql._mssql.raise_MSSQLDatabaseException\n",
      "pymssql._mssql.MSSQLDatabaseException: (20047, b'DB-Lib error message 20047, severity 9:\\nDBPROCESS is dead or not enabled\\n')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pymssql\n",
    "server = 'fs.rice.edu'\n",
    "database = 'stocks'\n",
    "username = 'stocks'\n",
    "password = '6LAZH1'\n",
    "string = \"mssql+pymssql://\" + username + \":\" + password + \"@\" + server + \"/\" + database \n",
    "conn = create_engine(string).connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculate financial ratios and growth rates\n",
    "\n",
    "Data from SF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sf1 = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, datekey, lastupdated, netinc, ncfo, equity, assets \n",
    "    from sf1\n",
    "    where dimension='ARQ' and datekey>='2021-01-01' and equity>0 and assets>0\n",
    "    order by ticker, datekey\n",
    "    \"\"\",\n",
    "    conn,\n",
    "    parse_dates=[\"datekey\"]\n",
    ")\n",
    "sf1 = sf1.groupby([\"ticker\", \"datekey\", \"lastupdated\"]).last()\n",
    "sf1 = sf1.droplevel(\"lastupdated\")\n",
    "sf1 = sf1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for col in [\"netinc\", \"ncfo\"]:\n",
    "    sf1[col] = sf1.groupby(\"ticker\", group_keys=False)[col].apply(\n",
    "        lambda x: x.rolling(4).sum()\n",
    "    )\n",
    "for col in [\"equity\", \"assets\"]:\n",
    "    sf1[col] = sf1.groupby(\"ticker\", group_keys=False)[col].apply(\n",
    "        lambda x: x.rolling(4).mean()\n",
    "    )\n",
    "sf1[\"roe\"] = sf1.netinc / sf1.equity\n",
    "sf1[\"accruals\"] = (sf1.netinc - sf1.ncfo) / sf1.equity\n",
    "sf1[\"agr\"] = sf1.groupby(\"ticker\", group_keys=False)[\"assets\"].pct_change()\n",
    "sf1 = sf1[[\"ticker\", \"datekey\", \"roe\", \"accruals\", \"agr\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Returns, volume, momentum, volatility\n",
    "\n",
    "Data from sep_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sep_weekly = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, date, volume, closeadj, closeunadj, lastupdated \n",
    "    from sep_weekly \n",
    "    where date>='2022-01-01'\n",
    "    order by ticker, date, lastupdated    \n",
    "    \"\"\",\n",
    "    conn,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "sep_weekly = sep_weekly.groupby([\"ticker\", \"date\", \"lastupdated\"]).last()\n",
    "sep_weekly = sep_weekly.droplevel(\"lastupdated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sep_weekly[\"ret\"] = sep_weekly.groupby(\"ticker\", group_keys=False).closeadj.pct_change()\n",
    "sep_weekly[\"annual\"] = sep_weekly.groupby(\"ticker\", group_keys=False).closeadj.pct_change(52)\n",
    "sep_weekly[\"monthly\"] = sep_weekly.groupby(\"ticker\", group_keys=False).closeadj.pct_change(4)\n",
    "sep_weekly[\"mom\"] = sep_weekly.groupby(\"ticker\", group_keys=False).apply(\n",
    "    lambda d: (1+d.annual)/(1+d.monthly) - 1\n",
    ")\n",
    "sep_weekly[\"volatility\"] = sep_weekly.groupby(\"ticker\", group_keys=False).ret.apply(\n",
    "    lambda x: x.rolling(26).std()\n",
    ")\n",
    "sep_weekly = sep_weekly[[\"mom\", \"volume\", \"volatility\", \"closeunadj\"]]\n",
    "sep_weekly = sep_weekly.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get marketcap and pb\n",
    "\n",
    "Data from weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "weekly = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, date, marketcap, pb, lastupdated\n",
    "    from weekly\n",
    "    where date>='2022-01-01' and marketcap>0 and pb>0\n",
    "    order by ticker, date, lastupdated\n",
    "    \"\"\",\n",
    "    conn,\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "weekly = weekly.groupby([\"ticker\", \"date\", \"lastupdated\"]).last()\n",
    "weekly = weekly.droplevel(\"lastupdated\")\n",
    "weekly = weekly.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = weekly.merge(sep_weekly, on=[\"ticker\", \"date\"], how=\"inner\")\n",
    "df[\"year\"] = df.date.apply(lambda x: x.isocalendar()[0])\n",
    "df[\"week\"] = df.date.apply(lambda x: x.isocalendar()[1])\n",
    "sf1[\"year\"] = sf1.datekey.apply(lambda x: x.isocalendar()[0])\n",
    "sf1[\"week\"] = sf1.datekey.apply(lambda x: x.isocalendar()[1])\n",
    "df = df.merge(sf1, on=[\"ticker\", \"year\", \"week\"], how=\"left\")\n",
    "df = df.drop(columns=[\"year\", \"week\", \"datekey\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fill ratios and growth rates forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for col in [\"roe\", \"accruals\", \"agr\"]:\n",
    "    df[col] = df.groupby(\"ticker\", group_keys=False)[col].apply(\n",
    "        lambda x: x.ffill()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Add sector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tickers = pd.read_sql(\n",
    "    \"\"\" \n",
    "    select ticker, sector from tickers\n",
    "    \"\"\",\n",
    "    conn\n",
    ")\n",
    "df = df.merge(tickers, on=\"ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filter to today's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df.date==df.date.max()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filter to small caps and exclude penny stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df.closeunadj>5]\n",
    "df = df.dropna()\n",
    "df[\"rnk\"] = df.marketcap.rank(\n",
    "    ascending=False, \n",
    "    method=\"first\"\n",
    ")\n",
    "df = df[(df.rnk>1000) & (df.rnk<=3000)]\n",
    "df = df.drop(columns=[\"closeunadj\", \"rnk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"marketcap\", \n",
    "    \"pb\", \n",
    "    \"mom\", \n",
    "    \"volume\", \n",
    "    \"volatility\", \n",
    "    \"roe\", \n",
    "    \"accruals\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# change this to \"./\" if forest.joblib is in your working directory\n",
    "path_to_file = \"../../\"\n",
    "\n",
    "from joblib import load\n",
    "forest = load(path_to_file + \"forest.joblib\")\n",
    "df[\"predict\"] = forest.predict(X=df[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find best and worst stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"rnk_long\"] = df.predict.rank(\n",
    "    ascending=False,\n",
    "    method=\"first\"\n",
    ")\n",
    "df[\"rnk_short\"] = df.predict.rank(\n",
    "    ascending=True,\n",
    "    method=\"first\"\n",
    ")\n",
    "longs = df[df.rnk_long<=44]\n",
    "shorts = df[df.rnk_short<=44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sector-neutral version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"rnk_long\"] = df.groupby(\"sector\", group_keys=False).predict.rank(\n",
    "    ascending=False,\n",
    "    method=\"first\"\n",
    ")\n",
    "df[\"rnk_short\"] = df.groupby(\"sector\", group_keys=False).predict.rank(\n",
    "    ascending=True,\n",
    "    method=\"first\"\n",
    ")\n",
    "longs_neutral = df[df.rnk_long<=4]\n",
    "shorts_neutral = df[df.rnk_short<=4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"portfolios 2023-11-08.xlsx\") as writer:\n",
    "    longs.to_excel(writer, \"long\", index=False)\n",
    "    shorts.to_excel(writer, \"short\", index=False)\n",
    "    longs_neutral.to_excel(writer, \"long neutral\", index=False)\n",
    "    shorts_neutral.to_excel(writer, \"short neutral\", index=False)\n",
    "    df.to_excel(writer, \"today\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
